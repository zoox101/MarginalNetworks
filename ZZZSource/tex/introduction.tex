In machine learning, increased accuracy is often paired with increased complexity. As technology continues to improve, new methods are being proposed to take advantage of recent advances in hardware and mathematics. Deep learning is supplanting shallow networks in fields such as image recognition \citep{Szegedy2015GoingConvolutions} and machine translation \citep{Bahdanau2014NeuralTranslate} while gradient boosted machines are replacing random forests in many tabular classification tasks \citep{Friedman2001GreedyMachine}. However, as machine learning techniques continue to improve, it is becoming increasingly difficult to understand each model’s inner workings. While in some applications this opacity is not an issue, in high-risk sectors such as medicine, law, and finance, model justifiability remains an extremely important feature \citep{Hastie2009TheLearning}.

Over the years, numerous techniques have been proposed to help unmask the underlying reasoning behind machine learning models. Some model-specific proposals take advantage of the method’s underlying structure, such as impurity importance in tree-based methods \citep{Louppe2013UnderstandingTrees} or network pruning in artificial neural networks \citep{Kingston2004AModelling}. Model-agnostic methods have also been proposed, such as partial dependence plots \citep{Friedman2001GreedyMachine}, sequential selection \citep{Zou2005RegularizationNet}, and permutation importance \citep{Breiman1984ClassificationTrees} to help determine the effect of one input on the model’s predicted output. While these techniques help with improving interpretability, none succeed in fully exposing the complete inner working of these complex techniques. 

A different approach is to create a model that’s interpretable by design. The earliest machine learning methods, ordinary least squares linear regression \citep{Galton1886RegressionStature.} and decision trees \citep{Belson1959MatchingClassification}, are good examples of this paradigm. While both techniques have severe limitations such as brittleness \citep{Norton1989GeneratingTrees} and strong assumptions \citep{Hayashi2000Econometrics}, both algorithms are still in common use, largely due to their simplicity and interpretability.  Statistics research has focused heavily on this paradigm, developing extensions to linear regression such as weighted least squares \citep{Suarez2017WeightedRegression} and logistic regression \citep{Nelder1972GeneralizedModels} to help address some of the concerns with ordinary least squares. These avenues have since coalesced into two paradigms, Generalized Linear Models (GLMs) \citep{Nelder1972GeneralizedModels} and Generalized Additive Models (GAMs) \citep{Hastie2007GeneralizedModels}, which strike a nice balance between power and interpretability, but come with their own set of drawbacks, such as complex domain-specific parametric tuning and little implementation support from the machine learning community.

This thesis proposes a novel machine learning method that fuses the natural interpretability of GAMs with the flexibility and support of neural networks. We have developed the first known open source GAM-like implementation in a neural network framework and demonstrated that the network’s natural flexibility allows the model to generalize better than traditional GAMs. Additionally, we have compared our model against other state-of-the-art classification techniques and demonstrated that we can achieve similar accuracy despite limitations on the network’s predictive power. Finally, we investigate some of the advantages and disadvantages of this this technique and discuss how it fits with other machine learning paradigms. 